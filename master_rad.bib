@misc{cheung_llama2_vs_gpt4_2023,
	author = {Cheung, D.},
	title = {Meta Llama 2 vs. OpenAI GPT-4: A Comparative Analysis},
	howpublished = {Medium},
	year = {2023}
}

@misc{anthropic_best_practices_2025,
	author = {{Anthropic}},
	title = {Using Anthropic: Best Practices, Parameters, and Large Context Windows},
	howpublished = {PromptHub Blog},
	year = {2025}
}

@misc{kramer_deepseek_2025,
	author = {Kramer, N.},
	title = {DeepSeek: Everything you need to know about this new LLM in one place},
	howpublished = {daily.dev},
	year = {2025}
}

@misc{openai_function_calling_2023,
	author = {{OpenAI}},
	title = {Function calling and other API updates},
	howpublished = {OpenAI News},
	year = {2023}
}

@misc{ibm_what_is_langchain_2023,
	author = {{IBM}},
	title = {What is LangChain?},
	howpublished = {IBM Think Blog},
	year = {2023}
}

@misc{patriwala_langchain_2025,
	author = {Patriwala, A.},
	title = {LangChain: A Comprehensive Framework for Building LLM Applications},
	howpublished = {Medium},
	year = {2025}
}

@misc{langchain_docs_2024,
	author = {{LangChain}},
	title = {LangChain Documentation},
	howpublished = {Online Documentation},
	url = {https://docs.langchain.com/},
	year = {2024}
}

@misc{pypi_edgar_2024,
	author = {{PyPI}},
	title = {edgar library, v5.6.3},
	howpublished = {Python Package Index (PyPI)},
	year = {2024}
}

@misc{sec_parser_docs_2023,
	author = {{sec-parser documentation}},
	title = {sec-parser: Semantic parsing for SEC filings},
	year = {2023}
}

@misc{sec_api_tutorial_2023,
	author = {{sec-api.io}},
	title = {Extract Textual Data from EDGAR 10-K Filings Using Python},
	howpublished = {Tutorial},
	year = {2023}
}

@misc{wikipedia_form_10k_2025,
	author = {{Wikipedia}},
	title = {Form 10-K},
	note = {приступ 2025},
	year = {2025}
}
@article{rocha_discovering_2021,
	author = {Rocha, Kelvin and others},
	title = {Discovering the sentiment in finance's unstructured data},
	publisher = {LSEG White Paper},
	year = {2021}
}

@article{paro_ai_strategic_2023,
	author = {{Paro AI}},
	title = {The Strategic Benefits of {NLP} in Finance},
	year = {2023}
}

@article{yang_evaluating_2025,
	author = {Yang, Z. and others},
	title = {Evaluating {LLMs} in Financial {NLP}: A Comparative Study on Financial Report Analysis},
	journalpublisher = {arXiv},
	year = {2025}
}

@misc{noauthor_artificial_20233,
	abstract = {An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often  monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.The artificial neuron transfer function should not be confused with a linear system's transfer function.
Artificial neurons can also refer to artificial cells in neuromorphic engineering (see below) that are similar to natural physical neurons.},
	copyright = {Creative Commons Attribution-ShareAlike License},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/GKD66JTQ/Artificial_neuron.html:text/html},
	journal = {Wikipedia},
	language = {en},
	month = aug,
	note = {Page Version ID: 1170144985},
	title = {Artificial neuron},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neuron&oldid=1170144985},
	urldate = {2023-09-21},
	year = {2023},
	bdsk-url-1 = {https://en.wikipedia.org/w/index.php?title=Artificial_neuron&oldid=1170144985}}

@phdthesis{ioannou_structural_2017,
	abstract = {Deep learning has in recent years come to dominate the previously separate fields of research in machine learning, computer vision, natural language understanding and speech recognition. Despite breakthroughs in training deep networks, there remains a lack of understanding of both the optimization and structure of deep networks. The approach advocated by many researchers in the field has been to train monolithic networks with excess complexity, and strong regularization --- an approach that leaves much to desire in efficiency. Instead we propose that carefully designing networks in consideration of our prior knowledge of the task and learned representation can improve the memory and compute efficiency of state-of-the art networks, and even improve generalization --- what we propose to denote as structural priors. We present two such novel structural priors for convolutional neural networks, and evaluate them in state-of-the-art image classification CNN architectures. The first of these methods proposes to exploit our knowledge of the low-rank nature of most filters learned for natural images by structuring a deep network to learn a collection of mostly small, low-rank, filters. The second addresses the filter/channel extents of convolutional filters, by learning filters with limited channel extents. The size of these channel-wise basis filters increases with the depth of the model, giving a novel sparse connection structure that resembles a tree root. Both methods are found to improve the generalization of these architectures while also decreasing the size and increasing the efficiency of their training and test-time computation. Finally, we present work towards conditional computation in deep neural networks, moving towards a method of automatically learning structural priors in deep networks. We propose a new discriminative learning model, conditional networks, that jointly exploit the accurate representation learning capabilities of deep neural networks with the efficient conditional computation of decision trees. Conditional networks yield smaller models, and offer test-time flexibility in the trade-off of computation vs. accuracy.},
	author = {Ioannou, Yani},
	doi = {10.17863/CAM.26357},
	file = {Full Text PDF:/Users/racicaleksa/Zotero/storage/LE7BK2Y2/Ioannou - 2017 - Structural Priors in Deep Neural Networks.pdf:application/pdf},
	month = sep,
	title = {Structural {Priors} in {Deep} {Neural} {Networks}},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.17863/CAM.26357}}

@article{anwar_learned_2017,
	abstract = {Ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as Human Computer Interaction (HCI); demographic based classification; biometric based recognition; security and defense to name a few. In this paper we present a novel approach for extracting ethnicity from the facial images. The proposed method makes use of a pre trained Convolutional Neural Network (CNN) to extract the features and then Support Vector Machine (SVM) with linear kernel is used as a classifier. This technique uses translational invariant hierarchical features learned by the network, in contrast to previous works, which use hand crafted features such as Local Binary Pattern (LBP); Gabor etc. Thorough experiments are presented on ten different facial databases which strongly suggest that our approach is robust to different expressions and illuminations conditions. Here we consider ethnicity classification as a three class problem including Asian, African-American and Caucasian. Average classification accuracy over all databases is 98.28\%, 99.66\% and 99.05\% for Asian, African-American and Caucasian respectively.},
	author = {Anwar, Inzamam and Ul Islam, Naeem},
	doi = {10.1515/cait-2017-0036},
	file = {Full Text:/Users/racicaleksa/Zotero/storage/6MAS4QC2/Anwar and Ul Islam - 2017 - Learned Features are Better for Ethnicity Classifi.pdf:application/pdf},
	journal = {Cybernetics and Information Technologies},
	month = sep,
	title = {Learned {Features} are {Better} for {Ethnicity} {Classification}},
	volume = {17},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1515/cait-2017-0036}}

@article{gage_new_1994,
	author = {Gage, Philip},
	title = {A New Algorithm for Data Compression},
	journal = {C Users Journal},
	volume = {12},
	number = {2},
	pages = {24--35},
	year = {1994}
}

@inproceedings{sennrich_neural_2016,
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	title = {Neural Machine Translation of Rare Words with Subword Units},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {1715--1725},
	year = {2016},
	publisher = {Association for Computational Linguistics}
}

@article{bengio_neural_2003,
	author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
	title = {A Neural Probabilistic Language Model},
	journal = {Journal of Machine Learning Research},
	volume = {3},
	pages = {1137--1155},
	year = {2003}
}

@article{bahdanau_neural_2015,
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	title = {Neural Machine Translation by Jointly Learning to Align and Translate},
	journal = {arXiv preprint arXiv:1409.0473},
	year = {2015}
}

@inproceedings{vaswani_attention_2017,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L{}ukasz and Polosukhin, Illia},
	title = {Attention Is All You Need},
	booktitle = {Advances in Neural Information Processing Systems 30},
	pages = {5998--6008},
	year = {2017},
	publisher = {Curran Associates, Inc.}
}

@techreport{radford_improving_2018,
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	title = {Improving Language Understanding by Generative Pre-Training},
	institution = {OpenAI},
	type = {Technical Report},
	year = {2018}
}

@techreport{radford_language_2019,
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	title = {Language Models are Unsupervised Multitask Learners},
	institution = {OpenAI},
	type = {Technical Report},
	year = {2019}
}

@inproceedings{brown_language_2020,
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	title = {Language Models are Few-Shot Learners},
	booktitle = {Advances in Neural Information Processing Systems 33},
	pages = {1877--1901},
	year = {2020},
	publisher = {Curran Associates, Inc.}
}

@misc{openai_gpt4_2023,
	author = {{OpenAI}},
	title = {{GPT-4} Technical Report},
	journal = {arXiv preprint arXiv:2303.08774},
	year = {2023}
}

@article{lu_large_2024,
	author = {Lu, Z.},
	title = {Large Language Models: Development in Model Scale and Challenges},
	journal = {Applied and Computational Engineering},
	volume = {114},
	pages = {154--161},
	year = {2024}
}

@misc{martineau_whats_2024,
	author = {Martineau, K.},
	title = {What's an {LLM} context window and why is it getting larger?},
	journal = {IBM Research Blog},
	year = {2024}
}

@misc{liu_lost_2023,
	author = {Liu, N. F. and Lin, K. and Hewitt, J. and Paranjape, A. and Bevilacqua, M. and Petroni, F. and Liang, P.},
	title = {Lost in the Middle: How Language Models Use Long Contexts},
	journal = {arXiv preprint arXiv:2307.03172},
	year = {2023}
}

@misc{sahoo_systematic_2025,
	author = {Sahoo, P. and Singh, A. K. and Saha, S. and Jain, V. and Mondal, S. and Chadha, A.},
	title = {A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications},
	journal = {arXiv preprint arXiv:2402.07927},
	year = {2025}
}

@inproceedings{lewis_retrieval_2020,
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"{u}}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"{a}}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	booktitle = {Advances in Neural Information Processing Systems 33},
	pages = {9459--9474},
	year = {2020},
	publisher = {Curran Associates, Inc.}
}

@article{yang_dual_2025,
	author = {Yang, Q. and others},
	title = {Dual Retrieving and Ranking Medical {LLM} with Retrieval Augmented Generation},
	journal = {Scientific Reports},
	volume = {15},
	pages = {18062},
	year = {2025}
}

@misc{kim_large_2024,
	author = {Kim, M.},
	title = {Large context windows versus {RAG}},
	journal = {IBM Research Blog -- Future Forward Series},
	year = {2024}
}

@misc{noauthor_artificial_2023,
	abstract = {Artificial neural networks (ANNs, also shortened to neural networks (NNs) or neural nets) are a branch of machine learning models that are built using principles of neuronal organization discovered by connectionism in the biological neural networks constituting animal brains.An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold.

Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.},
	copyright = {Creative Commons Attribution-ShareAlike License},
	file = {Snapshot:/Users/racicaleksa/Zotero/storage/3TA8ZASC/Artificial_neural_network.html:text/html},
	journal = {Wikipedia},
	language = {en},
	month = sep,
	note = {Page Version ID: 1175471829},
	title = {Artificial neural network},
	url = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1175471829},
	urldate = {2023-09-15},
	year = {2023},
	bdsk-url-1 = {https://en.wikipedia.org/w/index.php?title=Artificial_neural_network&oldid=1175471829}}

@misc{islam_financebench_2023,
	abstract = {FinanceBench is a new benchmark for evaluating the performance of LLMs on open book financial question answering (QA). The benchmark comprises 10,231 questions about publicly traded companies, with accompanying answers and evidence strings. The questions span 15+ years of financial data for more than 150+ companies and come from 10K and 10Q documents, earnings call transcripts, and investor materials. We test a comprehensive suite of state-of-the-art LLM architectures on FinanceBench, including both commercial APIs and open-source models. Our analysis reveals that even the best models struggle with complex reasoning tasks and questions that require understanding of financial context beyond what can be extracted from text alone. Our results suggest that financial QA is a challenging and relevant evaluation task for LLMs, highlighting important gaps in current capabilities.},
	author = {Islam, Pranab and Kannappan, Anupam and Kiela, Douwe and Qian, Rebecca and Scherrer, Nino and Vidgen, Bertie},
	doi = {10.48550/arXiv.2311.11944},
	file = {arXiv Fulltext PDF:/Users/racicaleksa/Zotero/storage/9XP2QRXY/Islam et al. - 2023 - FinanceBench A New Benchmark for Financial Questi.pdf:application/pdf;arXiv.org Snapshot:/Users/racicaleksa/Zotero/storage/HBXLVKX2/2311.html:text/html},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	note = {arXiv:2311.11944 [cs]},
	publisher = {arXiv},
	title = {{FinanceBench}: {A} {New} {Benchmark} for {Financial} {Question} {Answering}},
	url = {http://arxiv.org/abs/2311.11944},
	urldate = {2024-01-15},
	year = {2023},
	bdsk-url-1 = {http://arxiv.org/abs/2311.11944},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2311.11944}}

@misc{evidently_ai_llm_judge_2025,
	author = {{Evidently AI}},
	title = {LLM-as-a-judge: a complete guide to using LLMs for evaluations},
	howpublished = {Evidently AI Blog},
	year = {2025}
}

@article{zheng_judging_llm_2023,
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
	journal = {arXiv preprint arXiv:2306.05685},
	year = {2023}
}

@article{verga_replacing_judges_2024,
	author = {Verga, Pat and Elaraby, Nader and Joshi, Shantanu and Min, Sewon and Chen, Danqi and Ling, Xiao},
	title = {Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models},
	journal = {arXiv preprint arXiv:2404.18796},
	year = {2024}
}

@misc{clearwater_analytics_2023,
	author = {{Clearwater Analytics}},
	title = {A Cutting-Edge Framework for Evaluating LLM Output},
	howpublished = {Clearwater Analytics Blog},
	year = {2023}
}
